{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1f8b8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "# from selenium import webdriver\n",
    "# from webdriver_manager.chrome import ChromeDriverManager\n",
    "# from selenium.webdriver.chrome.options import Options\n",
    "# from selenium.webdriver.common.by import By\n",
    "# from selenium.webdriver.common.action_chains import ActionChains\n",
    "import requests\n",
    "import os\n",
    "import time\n",
    "from random_user_agent.user_agent import UserAgent\n",
    "from random_user_agent.params import SoftwareName, OperatingSystem\n",
    "from threading import Thread, Lock\n",
    "import threading\n",
    "import pickle\n",
    "# import undetected_chromedriver.v2 as uc\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e278ce",
   "metadata": {},
   "source": [
    "### РИА новости"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d0cc210d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохраняем всю ленту новостей\n",
    "\n",
    "def get_source_html(url, driver):\n",
    "    counter = 0\n",
    "    driver.get(url)\n",
    "\n",
    "    time.sleep(30)\n",
    "\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "    action = ActionChains(driver)\n",
    "    el = driver.find_element(By.CLASS_NAME, 'list-more')\n",
    "    action.move_to_element_with_offset(el, 5, 5)\n",
    "    action.click()\n",
    "    action.perform()\n",
    "\n",
    "    time.sleep(0.5)\n",
    "\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "    try:\n",
    "        counter += 2\n",
    "\n",
    "        while True:\n",
    "\n",
    "            if counter >= 300 and counter % 20 == 0:\n",
    "                with  open(f'ria_economy/raw_preview/news_ria{counter}.html', 'w') as file:\n",
    "                    file.write(driver.page_source)\n",
    "\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(0.5)\n",
    "            counter += 1\n",
    "\n",
    "    except Exception as _ex:\n",
    "        print(_ex)\n",
    "\n",
    "    finally:\n",
    "        driver.close()\n",
    "        driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a1b58a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сохраняем ссылки из ленты новостей\n",
    "\n",
    "def get_file_urls(file_path):\n",
    "    import os\n",
    "    files = sorted(os.listdir(file_path))[1:]\n",
    "\n",
    "    urls = []\n",
    "    c = 1\n",
    "    for f in files:\n",
    "\n",
    "        with open(file_path + f'/{f}') as file:\n",
    "            src = file.read()\n",
    "        \n",
    "        soup = BeautifulSoup(src, 'lxml')\n",
    "        items_divs = soup.find_all('div', class_='list-item')\n",
    "\n",
    "        for item in items_divs:\n",
    "            item_url = item.find('div', class_='list-item__content').find('a').get('href')\n",
    "            if item_url not in urls:\n",
    "                urls.append(item_url)\n",
    "\n",
    "        print(f'Ссылки из файлов {c} из {len(files)} получены')\n",
    "        c+=1\n",
    "\n",
    "    with open(f'ria_economy/urls.txt', 'w') as file:\n",
    "        for url in urls:\n",
    "            file.write(f'{url}\\n')\n",
    "\n",
    "        print(f'Ссылки сохранены')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "38fc4e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/m6/2n44mlkj7nbdwb651dd78pwc0000gn/T/ipykernel_810/1943467916.py:1: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(ChromeDriverManager().install())\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [20], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m driver \u001b[39m=\u001b[39m webdriver\u001b[39m.\u001b[39mChrome(ChromeDriverManager()\u001b[39m.\u001b[39minstall())\n\u001b[0;32m----> 3\u001b[0m get_source_html(url\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mhttps://ria.ru/politics/\u001b[39;49m\u001b[39m'\u001b[39;49m, driver\u001b[39m=\u001b[39;49mdriver)\n",
      "Cell \u001b[0;32mIn [17], line 26\u001b[0m, in \u001b[0;36mget_source_html\u001b[0;34m(url, driver)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39mif\u001b[39;00m counter \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m300\u001b[39m \u001b[39mand\u001b[39;00m counter \u001b[39m%\u001b[39m \u001b[39m20\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     25\u001b[0m     \u001b[39mwith\u001b[39;00m  \u001b[39mopen\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mria_economy/raw_preview/news_ria\u001b[39m\u001b[39m{\u001b[39;00mcounter\u001b[39m}\u001b[39;00m\u001b[39m.html\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m file:\n\u001b[0;32m---> 26\u001b[0m         file\u001b[39m.\u001b[39mwrite(driver\u001b[39m.\u001b[39;49mpage_source)\n\u001b[1;32m     28\u001b[0m driver\u001b[39m.\u001b[39mexecute_script(\u001b[39m\"\u001b[39m\u001b[39mwindow.scrollTo(0, document.body.scrollHeight);\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     29\u001b[0m time\u001b[39m.\u001b[39msleep(\u001b[39m0.5\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/selenium/webdriver/remote/webdriver.py:540\u001b[0m, in \u001b[0;36mWebDriver.page_source\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[1;32m    531\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpage_source\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[1;32m    532\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    533\u001b[0m \u001b[39m    Gets the source of the current page.\u001b[39;00m\n\u001b[1;32m    534\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    538\u001b[0m \u001b[39m            driver.page_source\u001b[39;00m\n\u001b[1;32m    539\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 540\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexecute(Command\u001b[39m.\u001b[39;49mGET_PAGE_SOURCE)[\u001b[39m'\u001b[39m\u001b[39mvalue\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/selenium/webdriver/remote/webdriver.py:427\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    424\u001b[0m         params[\u001b[39m'\u001b[39m\u001b[39msessionId\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msession_id\n\u001b[1;32m    426\u001b[0m params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wrap_value(params)\n\u001b[0;32m--> 427\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcommand_executor\u001b[39m.\u001b[39;49mexecute(driver_command, params)\n\u001b[1;32m    428\u001b[0m \u001b[39mif\u001b[39;00m response:\n\u001b[1;32m    429\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39merror_handler\u001b[39m.\u001b[39mcheck_response(response)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/selenium/webdriver/remote/remote_connection.py:344\u001b[0m, in \u001b[0;36mRemoteConnection.execute\u001b[0;34m(self, command, params)\u001b[0m\n\u001b[1;32m    342\u001b[0m data \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39mdump_json(params)\n\u001b[1;32m    343\u001b[0m url \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_url\u001b[39m}\u001b[39;00m\u001b[39m{\u001b[39;00mpath\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 344\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(command_info[\u001b[39m0\u001b[39;49m], url, body\u001b[39m=\u001b[39;49mdata)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/selenium/webdriver/remote/remote_connection.py:366\u001b[0m, in \u001b[0;36mRemoteConnection._request\u001b[0;34m(self, method, url, body)\u001b[0m\n\u001b[1;32m    363\u001b[0m     body \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    365\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkeep_alive:\n\u001b[0;32m--> 366\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conn\u001b[39m.\u001b[39;49mrequest(method, url, body\u001b[39m=\u001b[39;49mbody, headers\u001b[39m=\u001b[39;49mheaders)\n\u001b[1;32m    367\u001b[0m     statuscode \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mstatus\n\u001b[1;32m    368\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/urllib3/request.py:74\u001b[0m, in \u001b[0;36mRequestMethods.request\u001b[0;34m(self, method, url, fields, headers, **urlopen_kw)\u001b[0m\n\u001b[1;32m     71\u001b[0m urlopen_kw[\u001b[39m\"\u001b[39m\u001b[39mrequest_url\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m url\n\u001b[1;32m     73\u001b[0m \u001b[39mif\u001b[39;00m method \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_encode_url_methods:\n\u001b[0;32m---> 74\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest_encode_url(\n\u001b[1;32m     75\u001b[0m         method, url, fields\u001b[39m=\u001b[39;49mfields, headers\u001b[39m=\u001b[39;49mheaders, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49murlopen_kw\n\u001b[1;32m     76\u001b[0m     )\n\u001b[1;32m     77\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_encode_body(\n\u001b[1;32m     79\u001b[0m         method, url, fields\u001b[39m=\u001b[39mfields, headers\u001b[39m=\u001b[39mheaders, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39murlopen_kw\n\u001b[1;32m     80\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/urllib3/request.py:96\u001b[0m, in \u001b[0;36mRequestMethods.request_encode_url\u001b[0;34m(self, method, url, fields, headers, **urlopen_kw)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39mif\u001b[39;00m fields:\n\u001b[1;32m     94\u001b[0m     url \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m?\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m urlencode(fields)\n\u001b[0;32m---> 96\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49murlopen(method, url, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mextra_kw)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/urllib3/poolmanager.py:376\u001b[0m, in \u001b[0;36mPoolManager.urlopen\u001b[0;34m(self, method, url, redirect, **kw)\u001b[0m\n\u001b[1;32m    374\u001b[0m     response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39murlopen(method, url, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n\u001b[1;32m    375\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 376\u001b[0m     response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(method, u\u001b[39m.\u001b[39;49mrequest_uri, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    378\u001b[0m redirect_location \u001b[39m=\u001b[39m redirect \u001b[39mand\u001b[39;00m response\u001b[39m.\u001b[39mget_redirect_location()\n\u001b[1;32m    379\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m redirect_location:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/urllib3/connectionpool.py:703\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    700\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_proxy(conn)\n\u001b[1;32m    702\u001b[0m \u001b[39m# Make the request on the httplib connection object.\u001b[39;00m\n\u001b[0;32m--> 703\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    704\u001b[0m     conn,\n\u001b[1;32m    705\u001b[0m     method,\n\u001b[1;32m    706\u001b[0m     url,\n\u001b[1;32m    707\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    708\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    709\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    710\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    711\u001b[0m )\n\u001b[1;32m    713\u001b[0m \u001b[39m# If we're going to release the connection in ``finally:``, then\u001b[39;00m\n\u001b[1;32m    714\u001b[0m \u001b[39m# the response doesn't need to know about the connection. Otherwise\u001b[39;00m\n\u001b[1;32m    715\u001b[0m \u001b[39m# it will also try to release it and we'll have a double-release\u001b[39;00m\n\u001b[1;32m    716\u001b[0m \u001b[39m# mess.\u001b[39;00m\n\u001b[1;32m    717\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/urllib3/connectionpool.py:449\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    444\u001b[0m             httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39mgetresponse()\n\u001b[1;32m    445\u001b[0m         \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    446\u001b[0m             \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    447\u001b[0m             \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    448\u001b[0m             \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[0;32m--> 449\u001b[0m             six\u001b[39m.\u001b[39;49mraise_from(e, \u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    450\u001b[0m \u001b[39mexcept\u001b[39;00m (SocketTimeout, BaseSSLError, SocketError) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    451\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/urllib3/connectionpool.py:444\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    442\u001b[0m     \u001b[39m# Python 3\u001b[39;00m\n\u001b[1;32m    443\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 444\u001b[0m         httplib_response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[1;32m    445\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    446\u001b[0m         \u001b[39m# Remove the TypeError from the exception chain in\u001b[39;00m\n\u001b[1;32m    447\u001b[0m         \u001b[39m# Python 3 (including for exceptions like SystemExit).\u001b[39;00m\n\u001b[1;32m    448\u001b[0m         \u001b[39m# Otherwise it looks like a bug in the code.\u001b[39;00m\n\u001b[1;32m    449\u001b[0m         six\u001b[39m.\u001b[39mraise_from(e, \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/http/client.py:1347\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1345\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1346\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1347\u001b[0m         response\u001b[39m.\u001b[39;49mbegin()\n\u001b[1;32m   1348\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n\u001b[1;32m   1349\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/http/client.py:307\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[39m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    306\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 307\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n\u001b[1;32m    308\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n\u001b[1;32m    309\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/http/client.py:268\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_status\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 268\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadline(_MAXLINE \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m), \u001b[39m\"\u001b[39m\u001b[39miso-8859-1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    269\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n\u001b[1;32m    270\u001b[0m         \u001b[39mraise\u001b[39;00m LineTooLong(\u001b[39m\"\u001b[39m\u001b[39mstatus line\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/socket.py:669\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    668\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 669\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    670\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    671\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Запускаем функцию для сбора ссылок, останавливаем вручную, потому что нельзя листать ее до бесконечности.\n",
    "\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "\n",
    "get_source_html(url='https://ria.ru/politics/', driver=driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "df517986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ссылки из файлов 1 из 23 получены\n",
      "Ссылки из файлов 2 из 23 получены\n",
      "Ссылки из файлов 3 из 23 получены\n",
      "Ссылки из файлов 4 из 23 получены\n",
      "Ссылки из файлов 5 из 23 получены\n",
      "Ссылки из файлов 6 из 23 получены\n",
      "Ссылки из файлов 7 из 23 получены\n",
      "Ссылки из файлов 8 из 23 получены\n",
      "Ссылки из файлов 9 из 23 получены\n",
      "Ссылки из файлов 10 из 23 получены\n",
      "Ссылки из файлов 11 из 23 получены\n",
      "Ссылки из файлов 12 из 23 получены\n",
      "Ссылки из файлов 13 из 23 получены\n",
      "Ссылки из файлов 14 из 23 получены\n",
      "Ссылки из файлов 15 из 23 получены\n",
      "Ссылки из файлов 16 из 23 получены\n",
      "Ссылки из файлов 17 из 23 получены\n",
      "Ссылки из файлов 18 из 23 получены\n",
      "Ссылки из файлов 19 из 23 получены\n",
      "Ссылки из файлов 20 из 23 получены\n",
      "Ссылки из файлов 21 из 23 получены\n",
      "Ссылки из файлов 22 из 23 получены\n",
      "Ссылки из файлов 23 из 23 получены\n",
      "Ссылки сохранены\n"
     ]
    }
   ],
   "source": [
    "# Вытаскиваем ссылки\n",
    "file_path = '/Users/eugenborisenko/Desktop/Универ мага/Диссертация/Парсеры сайтов/ria_politics/raw_preview'\n",
    "get_file_urls(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14404aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем сохраненные ссылки\n",
    "with open('ria_politics/urls.txt') as file:\n",
    "    urls = file.read()\n",
    "urls = urls.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8497fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем набор user agent, чтобы сайт не так быстро блочил\n",
    "software_names = [SoftwareName.CHROME.value]\n",
    "operating_systems = [OperatingSystem.WINDOWS.value, OperatingSystem.LINUX.value]   \n",
    "\n",
    "user_agent_rotator = UserAgent(software_names=software_names, operating_systems=operating_systems, limit=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "052663f2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'urls' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m data \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mlink\u001b[39m\u001b[39m'\u001b[39m: [], \u001b[39m'\u001b[39m\u001b[39mdata_or_ex\u001b[39m\u001b[39m'\u001b[39m: []}\n\u001b[0;32m----> 2\u001b[0m total_news \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(urls)\n\u001b[1;32m      3\u001b[0m current_value \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'urls' is not defined"
     ]
    }
   ],
   "source": [
    "# Создаем массив данных, куда будут все склвдывать потоки\n",
    "data = {'link': [], 'data_or_ex': []}\n",
    "total_news = len(urls)\n",
    "current_value = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d31fbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Пишем функцию для парсинга нужных данных с многопоточностью.\n",
    "\n",
    "def parse_from_link(link, user_agent, current_value):\n",
    "    if current_value % 10 == 0:\n",
    "        user_agent = user_agent_rotator.get_random_user_agent()\n",
    "\n",
    "    try:\n",
    "        with requests.session() as s:\n",
    "            txt = s.get(link, headers={'User-Agent':user_agent}).text\n",
    "            soup = BeautifulSoup(txt, 'lxml')\n",
    "            try:\n",
    "                timestamp = soup.find('div', class_='article__info-date').find('a').text\n",
    "            except:\n",
    "                timestamp = 'No time'\n",
    "            \n",
    "            try:\n",
    "                title = soup.find('h1', class_='article__title').text\n",
    "            except:\n",
    "                title = 'No title'\n",
    "            try:\n",
    "                announce = soup.find('div', class_='article__announce-text').text\n",
    "            except:\n",
    "                announce = 'No announce'\n",
    "            try:\n",
    "                text = soup.find('div', class_='article__body js-mediator-article mia-analytics').text\n",
    "            except:\n",
    "                text = 'No text'\n",
    "            return timestamp, title, announce, text\n",
    "\n",
    "    except Exception as ex:\n",
    "        print(link)\n",
    "        print(ex)\n",
    "        return ex\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        return\n",
    "\n",
    "\n",
    "def parse(lock, links, save_evry):\n",
    "\n",
    "    global data, total_news, current_value\n",
    "\n",
    "    user_agent = user_agent_rotator.get_random_user_agent()\n",
    "\n",
    "    while current_value < total_news:\n",
    "        with lock:\n",
    "            thread_value = current_value\n",
    "            link = links[current_value]\n",
    "            current_value += 1\n",
    "        \n",
    "        parsed = parse_from_link(link, user_agent, thread_value)\n",
    "        if parsed == None:\n",
    "            return\n",
    "\n",
    "        with lock:\n",
    "            data['link'].append(link)\n",
    "            data['data_or_ex'].append(parsed)\n",
    "            if thread_value % save_evry == 0:\n",
    "                with open(f'ria_economy/raw_cites/{thread_value}_skipped.pkl', 'wb') as f:\n",
    "                    pickle.dump(data, f)\n",
    "                    print(f'{thread_value} saved')\n",
    "                data.clear()\n",
    "                data['link'] = []\n",
    "                data['data_or_ex'] = []\n",
    "\n",
    "        time.sleep(2)\n",
    "\n",
    "    with open(f'ria_economy/raw_cites/{thread_value}_skipped.pkl', 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "    return 'Конец'\n",
    "\n",
    "def start_parsing(links, n_threads=3, save_evry=15000):\n",
    "    \n",
    "    lock = Lock()\n",
    "    threads = []\n",
    "    run_event = threading.Event()\n",
    "    run_event.set()\n",
    "\n",
    "    for i in range(n_threads):\n",
    "        t = Thread(target=parse, args=(lock, links, save_evry))\n",
    "        t.start()\n",
    "        time.sleep(2)\n",
    "        threads.append(t)\n",
    "\n",
    "    for thread in threads:\n",
    "        thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620c7245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Запускаем функцию\n",
    "start_parsing(urls, n_threads=2, save_evry=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3db22d",
   "metadata": {},
   "source": [
    "### Дальше по сути тот же код, что и был выше, но я там докачивал пропуски"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3d58c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ria_politics/urls_retry.txt') as file:\n",
    "    urls = file.read()\n",
    "urls = urls.split('\\n')\n",
    "\n",
    "data = {'link': [], 'data_or_ex': []}\n",
    "total_news = len(urls)\n",
    "current_value = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4dd91fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 saved\n",
      "200 saved\n",
      "400 saved\n",
      "\n",
      "Invalid URL '': No scheme supplied. Perhaps you meant http://?\n"
     ]
    }
   ],
   "source": [
    "start_parsing(urls, n_threads=2, save_evry=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94b75015",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ria_economy/urls_retry.txt') as file:\n",
    "    urls = file.read()\n",
    "urls = urls.split('\\n')\n",
    "\n",
    "data = {'link': [], 'data_or_ex': []}\n",
    "total_news = len(urls)\n",
    "current_value = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43798cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 saved\n",
      "200 saved\n",
      "400 saved\n",
      "600 saved\n",
      "800 saved\n",
      "1000 saved\n",
      "1200 saved\n",
      "1400 saved\n",
      "1600 saved\n",
      "1800 saved\n",
      "2000 saved\n",
      "2200 saved\n",
      "2400 saved\n",
      "2600 saved\n",
      "2800 saved\n",
      "3000 saved\n",
      "\n",
      "Invalid URL '': No scheme supplied. Perhaps you meant http://?\n"
     ]
    }
   ],
   "source": [
    "start_parsing(urls, n_threads=2, save_evry=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e78b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ria_politics/urls_retry.txt') as file:\n",
    "    urls = file.read()\n",
    "urls = urls.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "205c08fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def urls_left(file_path):\n",
    "    urls = []\n",
    "    files = sorted(os.listdir(f'{file_path}'))\n",
    "    \n",
    "    for file in files:\n",
    "        try:\n",
    "            df = pd.DataFrame(pd.read_pickle(f'{file_path}/{file}'))\n",
    "            df['tp'] = df['data_or_ex'].map(lambda x: isinstance(x, tuple))\n",
    "            df.drop(df[df.tp == True].index, inplace=True)\n",
    "            df = df[df['link'].str.contains('https://ria.ru/')==True]\n",
    "            url = df['link'].astype(str).to_list()\n",
    "            urls.extend(url)\n",
    "        except Exception as ex:\n",
    "            print(ex)\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3731b4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "politics_path = '/Users/eugenborisenko/Desktop/Универ мага/Диссертация/Парсеры сайтов/ria_politics/raw_cites'\n",
    "economics_path = '/Users/eugenborisenko/Desktop/Универ мага/Диссертация/Парсеры сайтов/ria_economy/raw_cites'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fda35283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All arrays must be of the same length\n",
      "All arrays must be of the same length\n",
      "invalid load key, '\\x00'.\n",
      "Can only use .str accessor with string values!\n",
      "Can only use .str accessor with string values!\n",
      "Can only use .str accessor with string values!\n",
      "Can only use .str accessor with string values!\n",
      "Can only use .str accessor with string values!\n",
      "Can only use .str accessor with string values!\n",
      "Can only use .str accessor with string values!\n",
      "Can only use .str accessor with string values!\n",
      "Can only use .str accessor with string values!\n",
      "Can only use .str accessor with string values!\n"
     ]
    }
   ],
   "source": [
    "urls_politics = urls_left(politics_path)\n",
    "urls_economics = urls_left(economics_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4dc21b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'ria_politics/urls_retry.txt', 'w') as file:\n",
    "    for url in urls_politics:\n",
    "        file.write(f'{url}\\n')\n",
    "\n",
    "with open(f'ria_economy/urls_retry.txt', 'w') as file:\n",
    "    for url in urls_economics:\n",
    "        file.write(f'{url}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "984e27bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>link</th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>announce</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://ria.ru/20110211/333378123.html</td>\n",
       "      <td>19:57 11.02.2011</td>\n",
       "      <td>активист хабаровск приносить консульство япони...</td>\n",
       "      <td>активист молодежный движение хабаровск приноси...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://ria.ru/20110211/333385555.html</td>\n",
       "      <td>20:13 11.02.2011</td>\n",
       "      <td>токио протестовать против медведев снова поеха...</td>\n",
       "      <td>выражать серьезный протест отношение визит сос...</td>\n",
       "      <td>москва 11 фев риа новость япония категорически...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://ria.ru/20110211/333377675.html</td>\n",
       "      <td>19:56 11.02.2011</td>\n",
       "      <td>иран назло весь враг</td>\n",
       "      <td>иран отмечать 32 годовщина исламский революция...</td>\n",
       "      <td>иран отмечать 32 годовщина исламский революция...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://ria.ru/20110211/333366199.html</td>\n",
       "      <td>19:26 11.02.2011</td>\n",
       "      <td>российский нация должный состоять самобытный н...</td>\n",
       "      <td>большой задача касаться формирование будущий и...</td>\n",
       "      <td>уфа 11 фев риа новость президент рф дмитрий ме...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://ria.ru/20110211/333358165.html</td>\n",
       "      <td>19:05 11.02.2011</td>\n",
       "      <td>медведев называть межнациональный единство пер...</td>\n",
       "      <td>межнациональный единство мочь государство оно ...</td>\n",
       "      <td>уфа 11 фев риа новость руководитель весь регио...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>https://ria.ru/20110203/329935025.html</td>\n",
       "      <td>17:32 03.02.2011</td>\n",
       "      <td>рф подписывать соглашение ущерб свой безопасно...</td>\n",
       "      <td>разумеется договор сокращение наступательный в...</td>\n",
       "      <td>москва 3 фев риа новость весь решение сфера бе...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>https://ria.ru/20110203/329932778.html</td>\n",
       "      <td>17:27 03.02.2011</td>\n",
       "      <td>чиновник должный рассказывать свой работа теле...</td>\n",
       "      <td>считать представитель любой властный структура...</td>\n",
       "      <td>москва 3 фев риа новость премьер министр рф вл...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>https://ria.ru/20110203/329922066.html</td>\n",
       "      <td>16:59 03.02.2011</td>\n",
       "      <td>путин заявлять спокойно относиться критика вра...</td>\n",
       "      <td>слушать некоторый вещь знать самый дело жизнь ...</td>\n",
       "      <td>москва 3 фев риа новость премьер министр рф вл...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>https://ria.ru/20110203/329921841.html</td>\n",
       "      <td>16:58 03.02.2011</td>\n",
       "      <td>путин говорить дружить свой дочь</td>\n",
       "      <td>премьер признаваться удаваться дружить дочь ра...</td>\n",
       "      <td>москва 3 фев риа новость премьер министр рф вл...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>https://ria.ru/20110203/329915820.html</td>\n",
       "      <td>16:45 03.02.2011</td>\n",
       "      <td>силовик шокировать свой отдых нужно увольнять ...</td>\n",
       "      <td>человек который нести служба особый условие ри...</td>\n",
       "      <td>москва 3 фев риа новость правоохранительный ор...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>199 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       link              date  \\\n",
       "0    https://ria.ru/20110211/333378123.html  19:57 11.02.2011   \n",
       "1    https://ria.ru/20110211/333385555.html  20:13 11.02.2011   \n",
       "2    https://ria.ru/20110211/333377675.html  19:56 11.02.2011   \n",
       "3    https://ria.ru/20110211/333366199.html  19:26 11.02.2011   \n",
       "4    https://ria.ru/20110211/333358165.html  19:05 11.02.2011   \n",
       "..                                      ...               ...   \n",
       "194  https://ria.ru/20110203/329935025.html  17:32 03.02.2011   \n",
       "195  https://ria.ru/20110203/329932778.html  17:27 03.02.2011   \n",
       "196  https://ria.ru/20110203/329922066.html  16:59 03.02.2011   \n",
       "197  https://ria.ru/20110203/329921841.html  16:58 03.02.2011   \n",
       "198  https://ria.ru/20110203/329915820.html  16:45 03.02.2011   \n",
       "\n",
       "                                                 title  \\\n",
       "0    активист хабаровск приносить консульство япони...   \n",
       "1    токио протестовать против медведев снова поеха...   \n",
       "2                                 иран назло весь враг   \n",
       "3    российский нация должный состоять самобытный н...   \n",
       "4    медведев называть межнациональный единство пер...   \n",
       "..                                                 ...   \n",
       "194  рф подписывать соглашение ущерб свой безопасно...   \n",
       "195  чиновник должный рассказывать свой работа теле...   \n",
       "196  путин заявлять спокойно относиться критика вра...   \n",
       "197                   путин говорить дружить свой дочь   \n",
       "198  силовик шокировать свой отдых нужно увольнять ...   \n",
       "\n",
       "                                              announce  \\\n",
       "0    активист молодежный движение хабаровск приноси...   \n",
       "1    выражать серьезный протест отношение визит сос...   \n",
       "2    иран отмечать 32 годовщина исламский революция...   \n",
       "3    большой задача касаться формирование будущий и...   \n",
       "4    межнациональный единство мочь государство оно ...   \n",
       "..                                                 ...   \n",
       "194  разумеется договор сокращение наступательный в...   \n",
       "195  считать представитель любой властный структура...   \n",
       "196  слушать некоторый вещь знать самый дело жизнь ...   \n",
       "197  премьер признаваться удаваться дружить дочь ра...   \n",
       "198  человек который нести служба особый условие ри...   \n",
       "\n",
       "                                                  text  \n",
       "0                                                  NaN  \n",
       "1    москва 11 фев риа новость япония категорически...  \n",
       "2    иран отмечать 32 годовщина исламский революция...  \n",
       "3    уфа 11 фев риа новость президент рф дмитрий ме...  \n",
       "4    уфа 11 фев риа новость руководитель весь регио...  \n",
       "..                                                 ...  \n",
       "194  москва 3 фев риа новость весь решение сфера бе...  \n",
       "195  москва 3 фев риа новость премьер министр рф вл...  \n",
       "196  москва 3 фев риа новость премьер министр рф вл...  \n",
       "197  москва 3 фев риа новость премьер министр рф вл...  \n",
       "198  москва 3 фев риа новость правоохранительный ор...  \n",
       "\n",
       "[199 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('ria_politics/prep_cites/200.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a2bd092",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = []\n",
    "with open('/Users/eugenborisenko/Desktop/Универ мага/Диссертация/Парсеры сайтов/ria_economy/raw_preview/news_ria14oct15.html') as file:\n",
    "    src = file.read()\n",
    "\n",
    "soup = BeautifulSoup(src, 'lxml')\n",
    "items_divs = soup.find_all('div', class_='list-item')\n",
    "\n",
    "for item in items_divs:\n",
    "    item_url = item.find('div', class_='list-item__content').find('a').get('href')\n",
    "    if item_url not in urls:\n",
    "        urls.append(item_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b274c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'link': [], 'data_or_ex': []}\n",
    "total_news = len(urls)\n",
    "current_value = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4df9678c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 saved\n",
      "200 saved\n",
      "oil_cost_november_01112015\n",
      "Invalid URL 'oil_cost_november_01112015': No scheme supplied. Perhaps you meant http://oil_cost_november_01112015?\n",
      "currency_november_01112015\n",
      "Invalid URL 'currency_november_01112015': No scheme supplied. Perhaps you meant http://currency_november_01112015?\n",
      "stock_market_november_01112015\n",
      "Invalid URL 'stock_market_november_01112015': No scheme supplied. Perhaps you meant http://stock_market_november_01112015?\n",
      "400 saved\n",
      "600 saved\n",
      "summit_GECF_23112015\n",
      "Invalid URL 'summit_GECF_23112015': No scheme supplied. Perhaps you meant http://summit_GECF_23112015?\n",
      "800 saved\n",
      "1000 saved\n",
      "Manila_APEC_summit_17112015\n",
      "Invalid URL 'Manila_APEC_summit_17112015': No scheme supplied. Perhaps you meant http://Manila_APEC_summit_17112015?\n",
      "1200 saved\n",
      "1400 saved\n",
      "1600 saved\n",
      "1800 saved\n",
      "2000 saved\n",
      "2200 saved\n",
      "2400 saved\n",
      "2600 saved\n",
      "2800 saved\n",
      "3000 saved\n",
      "oil_cost_october_01102015\n",
      "Invalid URL 'oil_cost_october_01102015': No scheme supplied. Perhaps you meant http://oil_cost_october_01102015?\n",
      "currency_October_01102015\n",
      "Invalid URL 'currency_October_01102015': No scheme supplied. Perhaps you meant http://currency_October_01102015?\n",
      "stock_market_october_01102015\n",
      "Invalid URL 'stock_market_october_01102015': No scheme supplied. Perhaps you meant http://stock_market_october_01102015?\n",
      "3200 saved\n",
      "3400 saved\n",
      "3600 saved\n",
      "3800 saved\n",
      "4000 saved\n",
      "4200 saved\n",
      "4400 saved\n"
     ]
    }
   ],
   "source": [
    "start_parsing(urls, n_threads=2, save_evry=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8745f694",
   "metadata": {},
   "source": [
    "### Лента"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3c1d1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "software_names = [SoftwareName.CHROME.value]\n",
    "operating_systems = [OperatingSystem.WINDOWS.value, OperatingSystem.LINUX.value]   \n",
    "\n",
    "user_agent_rotator = UserAgent(software_names=software_names, operating_systems=operating_systems, limit=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ad385c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_urls(url, urls, user_agent):\n",
    "    page = 1\n",
    "    while True:\n",
    "        time.sleep(1.75)\n",
    "        url = url + f'page/{page}/'\n",
    "        data = requests.get(url, headers={'User-Agent':user_agent}).text\n",
    "        soup = BeautifulSoup(data, 'lxml')\n",
    "        items = soup.find_all('a', class_='card-full-news _archive')\n",
    "        if items == []:\n",
    "            break\n",
    "        for item in items:\n",
    "            item_url = 'https://lenta.ru/' + item.get('href')\n",
    "            if item_url not in urls:\n",
    "                urls.append(item_url)\n",
    "        page += 1\n",
    "    return urls\n",
    "    \n",
    "\n",
    "\n",
    "def get_lenta_urls():\n",
    "    c = 0\n",
    "    if c % 3 == 0:\n",
    "        user_agent = user_agent_rotator.get_random_user_agent()\n",
    "    urls = []\n",
    "    for year in range(2018, 2023):\n",
    "        for month in range(1, 13):\n",
    "            if month in [1, 3, 5, 7, 8, 10, 12]:\n",
    "                if len(str(month)) == 1:\n",
    "                    month = '0' + str(month)\n",
    "                for day in range(1, 32):\n",
    "                    if len(str(day)) == 1:\n",
    "                        day = '0' + str(day)\n",
    "                    url = f'https://lenta.ru/rubrics/economics/{year}/{month}/{day}/'\n",
    "                    urls = save_urls(url, urls, user_agent)\n",
    "            elif month in [4, 6, 9, 11]:\n",
    "                if len(str(month)) == 1:\n",
    "                    month = '0' + str(month)\n",
    "                for day in range(1, 31):\n",
    "                    if len(str(day)) == 1:\n",
    "                        day = '0' + str(day)\n",
    "                    url = f'https://lenta.ru/rubrics/economics/{year}/{month}/{day}/'\n",
    "                    urls = save_urls(url, urls, user_agent)\n",
    "            else:\n",
    "                if len(str(month)) == 1:\n",
    "                    month = '0' + str(month)\n",
    "                if year % 4 == 0:\n",
    "                    for day in range(1, 30):\n",
    "                        if len(str(day)) == 1:\n",
    "                            day = '0' + str(day)\n",
    "                        url = f'https://lenta.ru/rubrics/economics/{year}/{month}/{day}/'\n",
    "                        urls = save_urls(url, urls, user_agent)\n",
    "                else:\n",
    "                    for day in range(1, 29):\n",
    "                        if len(str(day)) == 1:\n",
    "                            day = '0' + str(day)\n",
    "                        url = f'https://lenta.ru/rubrics/economics/{year}/{month}/{day}/'\n",
    "                        urls = save_urls(url, urls, user_agent)\n",
    "                        \n",
    "            with open(f'lenta_economy/urls{year}-{month}.txt', 'w') as file:\n",
    "                for url in urls:\n",
    "                    file.write(f'{url}\\n')\n",
    "                urls = []\n",
    "            c += 1          \n",
    "            print(f'{year}-{month} готов')\n",
    "    print('Все сохранено')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92132be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-01 готов\n",
      "2018-02 готов\n",
      "2018-03 готов\n",
      "2018-04 готов\n",
      "2018-05 готов\n",
      "2018-06 готов\n"
     ]
    }
   ],
   "source": [
    "get_lenta_urls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cfc3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('ria_politics/urls.txt') as file:\n",
    "#     urls = file.read()\n",
    "# urls = urls.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b872774",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'link': [], 'data_or_ex': []}\n",
    "total_news = len(urls)\n",
    "current_value = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0492b34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_from_link_lenta(link, user_agent, current_value):\n",
    "    if current_value % 10 == 0:\n",
    "        user_agent = user_agent_rotator.get_random_user_agent()\n",
    "\n",
    "    try:\n",
    "        with requests.session() as s:\n",
    "            txt = s.get(link, headers={'User-Agent':user_agent}).text\n",
    "            soup = BeautifulSoup(txt, 'lxml')\n",
    "            try:\n",
    "                timestamp = soup.find('a', class_='topic-header__item topic-header__time').text\n",
    "            except:\n",
    "                timestamp = 'No time'\n",
    "            \n",
    "            try:\n",
    "                title = soup.find('span', class_='topic-body__title').text\n",
    "            except:\n",
    "                title = 'No title'\n",
    "            try:\n",
    "                announce = soup.find('div', class_='topic-body__title-yandex').text\n",
    "            except:\n",
    "                announce = 'No announce'\n",
    "            try:\n",
    "                text = soup.find_all('p', class_='topic-body__content-text')\n",
    "                text = ' '.join([el.text for el in text])\n",
    "            except:\n",
    "                text = 'No text'\n",
    "            return timestamp, title, announce, text\n",
    "\n",
    "    except Exception as ex:\n",
    "        print(link)\n",
    "        print(ex)\n",
    "        return ex\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df27f2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(lock, links, save_evry):\n",
    "\n",
    "    global data, total_news, current_value\n",
    "\n",
    "    user_agent = user_agent_rotator.get_random_user_agent()\n",
    "\n",
    "    while current_value < total_news:\n",
    "        with lock:\n",
    "            thread_value = current_value\n",
    "            link = links[current_value]\n",
    "            current_value += 1\n",
    "        \n",
    "        parsed = parse_from_link_lenta(link, user_agent, thread_value)\n",
    "        if parsed == None:\n",
    "            return\n",
    "\n",
    "        with lock:\n",
    "            data['link'].append(link)\n",
    "            data['data_or_ex'].append(parsed)\n",
    "            if thread_value % save_evry == 0:\n",
    "                with open(f'Парсеры сайтов/lenta_economy/raw_cites/{thread_value}.pkl', 'wb') as f:\n",
    "                    pickle.dump(data, f)\n",
    "                    print(f'{thread_value} saved')\n",
    "                data.clear()\n",
    "                data['link'] = []\n",
    "                data['data_or_ex'] = []\n",
    "\n",
    "        time.sleep(1.75)\n",
    "\n",
    "    with open(f'Парсеры сайтов/lenta_economy/raw_cites/{thread_value}.pkl', 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "    return 'Конец'\n",
    "\n",
    "def start_parsing(links, n_threads=2, save_evry=15000):\n",
    "    \n",
    "    lock = Lock()\n",
    "    threads = []\n",
    "    run_event = threading.Event()\n",
    "    run_event.set()\n",
    "\n",
    "    for i in range(n_threads):\n",
    "        t = Thread(target=parse, args=(lock, links, save_evry))\n",
    "        t.start()\n",
    "        time.sleep(1.75)\n",
    "        threads.append(t)\n",
    "\n",
    "    for thread in threads:\n",
    "        thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0223fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_parsing(urls, save_evry=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0632706",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd5913c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dad3c564",
   "metadata": {},
   "source": [
    "### РБК"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d8ea6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импорт библиотек\n",
    "import requests as rq\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e5f4943",
   "metadata": {},
   "outputs": [],
   "source": [
    "class rbc_parser:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def _get_url(self, param_dict: dict) -> str:\n",
    "        \"\"\"\n",
    "        Возвращает URL для запроса json таблицы со статьями\n",
    "        \"\"\"\n",
    "        url = 'https://www.rbc.ru/v10/search/ajax/?\\\n",
    "        project={0}&\\\n",
    "        category={1}&\\\n",
    "        dateFrom={2}&\\\n",
    "        dateTo={3}&\\\n",
    "        offset={4}&\\\n",
    "        limit={5}&\\\n",
    "        query={6}&\\\n",
    "        material={7}'.format(param_dict['project'],\n",
    "                            param_dict['category'],\n",
    "                            param_dict['dateFrom'],\n",
    "                            param_dict['dateTo'],\n",
    "                            param_dict['offset'],\n",
    "                            param_dict['limit'],\n",
    "                            param_dict['query'],\n",
    "                            param_dict['material'])\n",
    "        \n",
    "        return url\n",
    "    \n",
    "    \n",
    "    def _get_search_table(self, param_dict: dict,\n",
    "                          includeText: bool = True) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Возвращает pd.DataFrame со списком статей\n",
    "        \n",
    "        includeText: bool\n",
    "        ### Если True, статьи возвращаются с текстами\n",
    "        \"\"\"\n",
    "        url = self._get_url(param_dict)\n",
    "        r = rq.get(url)\n",
    "        search_table = pd.DataFrame(r.json()['items'])\n",
    "        if includeText and not search_table.empty:\n",
    "            get_text = lambda x: self._get_article_data(x['fronturl'])\n",
    "            search_table[['overview', 'text']] = search_table.apply(get_text,\n",
    "                                                                    axis=1).tolist()    \n",
    "        return search_table\n",
    "    \n",
    "    \n",
    "    def _get_article_data(self, url: str):\n",
    "        \"\"\"\n",
    "        Возвращает описание и текст статьи по ссылке\n",
    "        \"\"\"\n",
    "        r = rq.get(url)\n",
    "        soup = bs(r.text, features=\"lxml\") # features=\"lxml\" чтобы не было warning\n",
    "        div_overview = soup.find('div', {'class': 'article__text__overview'})\n",
    "        if div_overview:\n",
    "            overview = div_overview.text.replace('<br />','\\n').strip()\n",
    "        else:\n",
    "            overview = None\n",
    "        p_text = soup.find_all('p')\n",
    "        if p_text:\n",
    "            text = ' '.join(map(lambda x:\n",
    "                                x.text.replace('<br />','\\n').strip(),\n",
    "                                p_text))\n",
    "        else:\n",
    "            text = None\n",
    "        \n",
    "        return overview, text \n",
    "    \n",
    "    def get_articles(self,\n",
    "                     param_dict,\n",
    "                     time_step = 7,\n",
    "                     save_every = 5,\n",
    "                     save_excel = True) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Функция для скачивания статей интервалами через каждые time_step дней\n",
    "        Делает сохранение таблицы через каждые save_every * time_step дней\n",
    "\n",
    "        param_dict: dict\n",
    "        ### Параметры запроса \n",
    "        ###### project - раздел поиска, например, rbcnews\n",
    "        ###### category - категория поиска, например, TopRbcRu_economics\n",
    "        ###### dateFrom - с даты\n",
    "        ###### dateTo - по дату\n",
    "        ###### offset - смещение поисковой выдачи\n",
    "        ###### limit - лимит статей, максимум 100\n",
    "        ###### query - поисковой запрос (ключевое слово), например, РБК\n",
    "\n",
    "        \"\"\"\n",
    "        param_copy = param_dict.copy()\n",
    "        time_step = timedelta(days=time_step)\n",
    "        dateFrom = datetime.strptime(param_copy['dateFrom'], '%d.%m.%Y')\n",
    "        dateTo = datetime.strptime(param_copy['dateTo'], '%d.%m.%Y')\n",
    "        if dateFrom > dateTo:\n",
    "            raise ValueError('dateFrom should be less than dateTo')\n",
    "        \n",
    "        out = pd.DataFrame()\n",
    "        save_counter = 0\n",
    "\n",
    "        while dateFrom <= dateTo:\n",
    "            param_copy['dateTo'] = (dateFrom + time_step).strftime(\"%d.%m.%Y\")\n",
    "            if dateFrom + time_step > dateTo:\n",
    "                param_copy['dateTo'] = dateTo.strftime(\"%d.%m.%Y\")\n",
    "            print('Parsing articles from ' + param_copy['dateFrom'] +  ' to ' + param_copy['dateTo'])\n",
    "            out = out.append(self._get_search_table(param_copy), ignore_index=True)\n",
    "            dateFrom += time_step + timedelta(days=1)\n",
    "            param_copy['dateFrom'] = dateFrom.strftime(\"%d.%m.%Y\")\n",
    "            save_counter += 1\n",
    "            if save_counter == save_every:\n",
    "                display.clear_output(wait=True)\n",
    "                out.to_csv(\"checkpoint_table.csv\")\n",
    "                print('Checkpoint saved!')\n",
    "                save_counter = 0\n",
    "        \n",
    "        if save_excel:\n",
    "            out.to_csv(\"rbc_{}_{}.csv\".format(\n",
    "                param_dict['dateFrom'],\n",
    "                param_dict['dateTo']))\n",
    "        print('Finish')\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "960db4e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "РБК - param_dict: {'query': 'РБК', 'project': '', 'category': 'economics', 'dateFrom': '01.01.2021', 'dateTo': '28.02.2021', 'offset': '0', 'limit': '5', 'material': ''}\n"
     ]
    }
   ],
   "source": [
    "# Задаем параметры запросы и складываем в param_dict\n",
    "use_parser = \"РБК\"\n",
    "\n",
    "query = 'РБК'\n",
    "project = \"\"\n",
    "category = \"economics\"\n",
    "material = \"\"\n",
    "dateFrom = '2021-01-01'\n",
    "dateTo = \"2021-02-28\"\n",
    "offset = 0\n",
    "limit = 100\n",
    "\n",
    "if use_parser == \"РБК\":\n",
    "    param_dict = {'query'   : query, \n",
    "                  'project' : project,\n",
    "                  'category': category,\n",
    "                  'dateFrom': datetime.\n",
    "                  strptime(dateFrom, '%Y-%m-%d').\n",
    "                  strftime('%d.%m.%Y'),\n",
    "                  'dateTo'  : datetime.\n",
    "                  strptime(dateTo, '%Y-%m-%d').\n",
    "                  strftime('%d.%m.%Y'),\n",
    "                  'offset'  : str(offset),\n",
    "                  'limit'   : str(limit),\n",
    "                  'material': material}\n",
    "\n",
    "print(use_parser, \"- param_dict:\", param_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b9e635e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>fronturl</th>\n",
       "      <th>publish_date_t</th>\n",
       "      <th>publish_date</th>\n",
       "      <th>title</th>\n",
       "      <th>photo</th>\n",
       "      <th>project</th>\n",
       "      <th>category</th>\n",
       "      <th>opinion_authors</th>\n",
       "      <th>authors</th>\n",
       "      <th>anons</th>\n",
       "      <th>overview</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63b308ac9a7947e7e48fbbf1</td>\n",
       "      <td>https://www.rbc.ru/rbcfreenews/63b308ac9a7947e...</td>\n",
       "      <td>1672680747</td>\n",
       "      <td>Mon, 02 Jan 2023 20:32:27 +0300</td>\n",
       "      <td>Военный комитет НАТО обсудит конфликты на Укра...</td>\n",
       "      <td>{'url': ''}</td>\n",
       "      <td>None</td>\n",
       "      <td>Политика</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...  «обеспечении большей стабильности, созда...</td>\n",
       "      <td>None</td>\n",
       "      <td>Высший военный орган НАТО, Военный комитет, пр...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>63b308a49a7947e7e48fbbef</td>\n",
       "      <td>https://www.rbc.ru/politics/02/01/2023/63b308a...</td>\n",
       "      <td>1672679469</td>\n",
       "      <td>Mon, 02 Jan 2023 20:11:09 +0300</td>\n",
       "      <td>Самарский губернатор рассказал о погибших при ...</td>\n",
       "      <td>{'url': 'https://s0.rbk.ru/v6_top_pics/resized...</td>\n",
       "      <td>None</td>\n",
       "      <td>Политика</td>\n",
       "      <td>None</td>\n",
       "      <td>[Владислав Гордеев]</td>\n",
       "      <td>... . Семьи военнослужащих могут рассчитывать...</td>\n",
       "      <td>При ракетном ударе по Макеевке погибли в том ч...</td>\n",
       "      <td>Губернатор Самарской области Дмитрий Азаров со...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>621a39ba9a79472784f029d4</td>\n",
       "      <td>https://www.rbc.ru/politics/02/01/2023/621a39b...</td>\n",
       "      <td>1672678252</td>\n",
       "      <td>Mon, 02 Jan 2023 19:50:52 +0300</td>\n",
       "      <td>Военная операция на Украине. Карта</td>\n",
       "      <td>{'url': ''}</td>\n",
       "      <td>None</td>\n",
       "      <td>Политика</td>\n",
       "      <td>None</td>\n",
       "      <td>[Редакция РБК]</td>\n",
       "      <td>...  один указ президента устанавливает разны...</td>\n",
       "      <td>Россия начала военную операцию на Украине 24 ф...</td>\n",
       "      <td>2 января Минобороны России сообщило об ударе ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>63b2fe8a9a7947e6a2883245</td>\n",
       "      <td>https://www.rbc.ru/rbcfreenews/63b2fe8a9a7947e...</td>\n",
       "      <td>1672677400</td>\n",
       "      <td>Mon, 02 Jan 2023 19:36:40 +0300</td>\n",
       "      <td>Суд арестовал обвиняемого в убийстве сотрудник...</td>\n",
       "      <td>{'url': ''}</td>\n",
       "      <td>None</td>\n",
       "      <td>Общество</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...  нападавший 16 раз ударил погибшего ножом...</td>\n",
       "      <td>None</td>\n",
       "      <td>Тверской суд Москвы арестовал обвиняемого в у...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>63b2ff8b9a7947e649b8b743</td>\n",
       "      <td>https://www.rbc.ru/politics/02/01/2023/63b2ff8...</td>\n",
       "      <td>1672676691</td>\n",
       "      <td>Mon, 02 Jan 2023 19:24:51 +0300</td>\n",
       "      <td>Принц Гарри заявил о желании вернуть отца и брата</td>\n",
       "      <td>{'url': 'https://s0.rbk.ru/v6_top_pics/resized...</td>\n",
       "      <td>None</td>\n",
       "      <td>Политика</td>\n",
       "      <td>None</td>\n",
       "      <td>[Владислав Гордеев]</td>\n",
       "      <td>...  не институцию». Полностью интервью будет...</td>\n",
       "      <td>Принц Гарри и его жена не снижают уровень меди...</td>\n",
       "      <td>Герцог Сассекский принц Гарри считает, что кор...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id  \\\n",
       "0  63b308ac9a7947e7e48fbbf1   \n",
       "1  63b308a49a7947e7e48fbbef   \n",
       "2  621a39ba9a79472784f029d4   \n",
       "3  63b2fe8a9a7947e6a2883245   \n",
       "4  63b2ff8b9a7947e649b8b743   \n",
       "\n",
       "                                            fronturl publish_date_t  \\\n",
       "0  https://www.rbc.ru/rbcfreenews/63b308ac9a7947e...     1672680747   \n",
       "1  https://www.rbc.ru/politics/02/01/2023/63b308a...     1672679469   \n",
       "2  https://www.rbc.ru/politics/02/01/2023/621a39b...     1672678252   \n",
       "3  https://www.rbc.ru/rbcfreenews/63b2fe8a9a7947e...     1672677400   \n",
       "4  https://www.rbc.ru/politics/02/01/2023/63b2ff8...     1672676691   \n",
       "\n",
       "                      publish_date  \\\n",
       "0  Mon, 02 Jan 2023 20:32:27 +0300   \n",
       "1  Mon, 02 Jan 2023 20:11:09 +0300   \n",
       "2  Mon, 02 Jan 2023 19:50:52 +0300   \n",
       "3  Mon, 02 Jan 2023 19:36:40 +0300   \n",
       "4  Mon, 02 Jan 2023 19:24:51 +0300   \n",
       "\n",
       "                                               title  \\\n",
       "0  Военный комитет НАТО обсудит конфликты на Укра...   \n",
       "1  Самарский губернатор рассказал о погибших при ...   \n",
       "2                 Военная операция на Украине. Карта   \n",
       "3  Суд арестовал обвиняемого в убийстве сотрудник...   \n",
       "4  Принц Гарри заявил о желании вернуть отца и брата   \n",
       "\n",
       "                                               photo project  category  \\\n",
       "0                                        {'url': ''}    None  Политика   \n",
       "1  {'url': 'https://s0.rbk.ru/v6_top_pics/resized...    None  Политика   \n",
       "2                                        {'url': ''}    None  Политика   \n",
       "3                                        {'url': ''}    None  Общество   \n",
       "4  {'url': 'https://s0.rbk.ru/v6_top_pics/resized...    None  Политика   \n",
       "\n",
       "  opinion_authors              authors  \\\n",
       "0            None                 None   \n",
       "1            None  [Владислав Гордеев]   \n",
       "2            None       [Редакция РБК]   \n",
       "3            None                 None   \n",
       "4            None  [Владислав Гордеев]   \n",
       "\n",
       "                                               anons  \\\n",
       "0   ...  «обеспечении большей стабильности, созда...   \n",
       "1   ... . Семьи военнослужащих могут рассчитывать...   \n",
       "2   ...  один указ президента устанавливает разны...   \n",
       "3   ...  нападавший 16 раз ударил погибшего ножом...   \n",
       "4   ...  не институцию». Полностью интервью будет...   \n",
       "\n",
       "                                            overview  \\\n",
       "0                                               None   \n",
       "1  При ракетном ударе по Макеевке погибли в том ч...   \n",
       "2  Россия начала военную операцию на Украине 24 ф...   \n",
       "3                                               None   \n",
       "4  Принц Гарри и его жена не снижают уровень меди...   \n",
       "\n",
       "                                                text  \n",
       "0  Высший военный орган НАТО, Военный комитет, пр...  \n",
       "1  Губернатор Самарской области Дмитрий Азаров со...  \n",
       "2   2 января Минобороны России сообщило об ударе ...  \n",
       "3   Тверской суд Москвы арестовал обвиняемого в у...  \n",
       "4  Герцог Сассекский принц Гарри считает, что кор...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Пример того, как выглядит json таблица запроса по параметрам.\n",
    "# Действует ограничение в 100 статей на 1 запрос (параметром limit)\n",
    "assert use_parser == \"РБК\"\n",
    "parser = rbc_parser()\n",
    "tbl = parser._get_search_table(param_dict,\n",
    "                               includeText = True) # Парсить текст статей\n",
    "print(len(tbl))\n",
    "tbl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4781d370",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = parser.get_articles(param_dict=param_dict,\n",
    "                             time_step = 7, # Шаг - 7 дней, можно больше,\n",
    "                                            # но есть риск отсечения статей в неделях, гдестатей больше 100\n",
    "                             save_every = 5, # Сохранять чекпойнт каждые 5 шагов\n",
    "                             save_excel = True) # Сохранить итоговый файл\n",
    "print(len(table))\n",
    "table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a162696",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "81af195c",
   "metadata": {},
   "source": [
    "### Ведомости"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b96f2b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_urls(file_path):\n",
    "    import os\n",
    "    files = sorted(os.listdir(file_path))\n",
    "    files = [file for file in files if 'html' in file]\n",
    "\n",
    "    urls = []\n",
    "    c = 1\n",
    "    for f in files:\n",
    "\n",
    "        with open(file_path + f'/{f}', encoding='utf-8') as file:\n",
    "            src = file.read()\n",
    "        \n",
    "        soup = BeautifulSoup(src, 'lxml')\n",
    "        items_divs = soup.find_all('a', class_=\"article-preview-item articles-preview-list__item\")\n",
    "\n",
    "        for item in items_divs:\n",
    "            item_url = item.get('href')\n",
    "            if item_url not in urls:\n",
    "                urls.append(item_url)\n",
    "\n",
    "        print(f'Ссылки из файлов {c} из {len(files)} получены')\n",
    "        c+=1\n",
    "        with open(f'vedomosti_invest/urls_{f[10:-5]}.txt', 'w') as file:\n",
    "            for url in urls:\n",
    "                file.write(f'{url}\\n')\n",
    "            urls = []\n",
    "\n",
    "        print(f'Ссылки сохранены')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "410701fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ссылки из файлов 1 из 1 получены\n",
      "Ссылки сохранены\n"
     ]
    }
   ],
   "source": [
    "get_file_urls('vedomosti_business')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2dff8d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_from_link_vedomosti(link, user_agent, current_value):\n",
    "    if current_value % 10 == 0:\n",
    "        user_agent = user_agent_rotator.get_random_user_agent()\n",
    "\n",
    "    try:\n",
    "        with requests.session() as s:\n",
    "            txt = s.get(link, headers={'User-Agent':user_agent}).text\n",
    "            soup = BeautifulSoup(txt, 'lxml')\n",
    "            try:\n",
    "                timestamp = soup.find('time', class_='article-meta__date').get('datetime')\n",
    "            except:\n",
    "                timestamp = 'No time'\n",
    "            \n",
    "            try:\n",
    "                title = soup.find('h1', class_='article-headline__title').text\n",
    "            except:\n",
    "                title = 'No title'\n",
    "            try:\n",
    "                announce = soup.find('em', class_='article-headline__subtitle').text\n",
    "            except:\n",
    "                announce = 'No announce'\n",
    "            try:\n",
    "                text = soup.find_all('p', class_='box-paragraph__text')\n",
    "                text = ' '.join([el.text for el in text])\n",
    "            except:\n",
    "                text = 'No text'\n",
    "            return timestamp, title, announce, text\n",
    "\n",
    "    except Exception as ex:\n",
    "        print(link)\n",
    "        print(ex)\n",
    "        return ex\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ec9295e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(lock, links, save_evry):\n",
    "\n",
    "    global data, total_news, current_value\n",
    "\n",
    "    user_agent = user_agent_rotator.get_random_user_agent()\n",
    "\n",
    "    while current_value < total_news:\n",
    "        with lock:\n",
    "            thread_value = current_value\n",
    "            link = links[current_value]\n",
    "            current_value += 1\n",
    "        \n",
    "        parsed = parse_from_link_vedomosti(link, user_agent, thread_value)\n",
    "        if parsed == None:\n",
    "            return\n",
    "\n",
    "        with lock:\n",
    "            data['link'].append(link)\n",
    "            data['data_or_ex'].append(parsed)\n",
    "            if thread_value % save_evry == 0:\n",
    "                with open(f'vedomosti_invest/raw_cites/{thread_value}.pkl', 'wb') as f:\n",
    "                    pickle.dump(data, f)\n",
    "                    print(f'{thread_value} saved')\n",
    "                data.clear()\n",
    "                data['link'] = []\n",
    "                data['data_or_ex'] = []\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "    with open(f'vedomosti_invest/raw_cites/{thread_value}.pkl', 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "\n",
    "    return 'Конец'\n",
    "\n",
    "def start_parsing(links, n_threads=2, save_evry=15000):\n",
    "    \n",
    "    lock = Lock()\n",
    "    threads = []\n",
    "    run_event = threading.Event()\n",
    "    run_event.set()\n",
    "\n",
    "    for i in range(n_threads):\n",
    "        t = Thread(target=parse, args=(lock, links, save_evry))\n",
    "        t.start()\n",
    "        time.sleep(1)\n",
    "        threads.append(t)\n",
    "\n",
    "    for thread in threads:\n",
    "        thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d67e06f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "software_names = [SoftwareName.CHROME.value]\n",
    "operating_systems = [OperatingSystem.WINDOWS.value, OperatingSystem.LINUX.value]   \n",
    "\n",
    "user_agent_rotator = UserAgent(software_names=software_names, operating_systems=operating_systems, limit=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b8fcb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vedomosti_invest/urls_invest.txt') as file:\n",
    "    urls = file.read()\n",
    "urls = urls.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6329d36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'link': [], 'data_or_ex': []}\n",
    "total_news = len(urls)\n",
    "current_value = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70d6a682",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "193"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "646e48af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 saved\n",
      "\n",
      "Invalid URL '': No scheme supplied. Perhaps you meant http://?\n"
     ]
    }
   ],
   "source": [
    "start_parsing(urls, n_threads=2, save_evry=200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "0adcc2737ebf6a4a119f135174df96668767fca1ef1112612db5ecadf2b6d608"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
